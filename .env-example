# ==============================================================================
# Environment Configuration
#
# Instructions:
# 1. Create a copy of this file named `.env`.
# 2. Fill in the values for each variable below.
# ==============================================================================

# --- Model Backend Selection ---
# Specifies the backend for the generative models.
# Set to 1 to use Google Cloud Vertex AI (recommended).
# Set to 0 to use Google AI Studio API key (requires a GOOGLE_API_KEY).
GOOGLE_GENAI_USE_VERTEXAI=1

# --- ML Dev Backend Configuration ---
# Required only if GOOGLE_GENAI_USE_VERTEXAI is set to 0.
# Your Google AI Studio API key.
GOOGLE_API_KEY='<your-key-here>'

# --- Vertex AI Backend Configuration ---
# The Google Cloud Project ID to use for Vertex AI services.
GOOGLE_CLOUD_PROJECT='<your-project>'
# The Google Cloud region where your Vertex AI resources are located. 
GOOGLE_CLOUD_LOCATION='<your-location>'

# --- Agent Configuration ---
# These values define the identity of your agent. They are used in two key places: 
# 1. In `agent.py` for the `name` and `description` of the root agent object.   
# 2. In `deployment/deploy.py` for the `display_name` and `description` of the deployed Vertex AI Agent Engine. 
# 3. In `tools.py` and other modules, `display_name` is used as a prefix for logging messages to make you easier to track.
AGENT_DISPLAY_NAME='Data_Agent' #Agent name should start with a letter (a-z, A-Z) or an underscore (_), and can only contain letters, digits (0-9), and underscores. 
AGENT_DESCRIPTION='An agent that can answer questions about data in BigQuery.'
# Specify the generative models to be used by data agent
DATA_AGENT_MODEL='gemini-2.5-pro'
VIS_AGENT_MODEL='gemini-2.5-flash'
# In some cases you may want to separate the BigQuery compute consumption from BigQuery data storage. 
# You can set BQ_DATA_PROJECT_ID to the project you use for data storage, and BQ_COMPUTE_PROJECT_ID to the project you want to use for compute.
# Otherwise, you can set both BQ_DATA_PROJECT_ID and BQ_COMPUTE_PROJECT_ID to the same project id.
BQ_DATA_PROJECT_ID='<your-project>' # The Google Cloud Project ID that contains the BigQuery datasets and tables to be analyzed. (e.g., "my-gcp-project-123")
BQ_COMPUTE_PROJECT_ID='<your-project>' # The Google Cloud Project ID that you want to use for compute. 
BQ_DATASET_NAME='<your-dataset>' # The target BigQuery dataset name to be analyzed or for which data profiles are fetched. (e.g., "sales_data")
BQ_LOCATION='<your-location>' # The geographical location of the BigQuery datasets and tables to be analyzed (e.g., "US", "asia-northeast3")
BQ_TABLE_NAMES='' # Optional: specific table names within DATASET_NAME. Use a comma-separated string.(e.g., "table1,table2,table3") If empty, operations might apply to all tables in the dataset.
DATA_PROFILES_TABLE_FULL_ID='' # Optional: Full BigQuery table ID where data profiling results are stored. Set to None or an empty string if not used. (e.g., "my_project.profiling_dataset.all_profiles", None, "")
FEW_SHOT_EXAMPLES_TABLE_FULL_ID=''
ASPECT_TYPES='' # Optional: Comma-separated list of custom aspect type IDs to fetch from Dataplex. If this is empty, no aspects will be fetched. (e.g., "aspect1,aspect2")

# --- BigQuery Authentication Configuration ---
# Defines the authentication method for accessing BigQuery. 
# Set to 'OAUTH2' for an interactive, user-based authentication flow.
# Set to 'None' or leave empty to use Application Default Credentials (ADC)
BQ_CREDENTIALS_TYPE="None" #Either 'None' or 'OAUTH2' 

# --- Bigquery OAuth 2.0 Configuration ---
# These values are REQUIRED if BQ_CREDENTIALS_TYPE is set to 'OAUTH2'.
# You can obtain them by creating an OAuth 2.0 Client ID in the Google Cloud
# Console under "APIs & Services" -> "Credentials" -> "Create credentials"
OAUTH_CLIENT_ID=''
OAUTH_CLIENT_SECRET=''